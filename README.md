# Getting started: MPCDF Raven #

This tutorial is written for the MPCDF Raven users from the MPI of Animal Behavior. It focuses on using R on Raven and provides specific instructions on how to run jobs that require popular spatio-temporal R packages. This tutorial has been put together and is maintained by the [Animal-Environment Interactions research group](https://www.ab.mpg.de/safi).

The Raven user guide can be found [here](https://docs.mpcdf.mpg.de/doc/computing/raven-user-guide.html#login)

## Overview of the workflow ##
add text here ;)

To make the whole process more enjoyable for yourself, have a look at these tutorials on [directory management in Unix](https://www.tutorialspoint.com/unix/unix-directories.htm) and [basics of shell scripting](https://www.tutorialspoint.com/unix/unix-what-is-shell.htm) if these topics are new to you.

## Step 1: Create an account ##

1. Go to https://selfservice.mpcdf.mpg.de/register/antrag.php?lang=en
2. Select MPI of Animal Behavior
3. Pick one person to approve your application


## Step 2: Set up Two-factor authentication (2FA) ##

1. Install a 2FA app on your mobile phone (e.g. Aegis Authenticator, andOTP, FreeOTP, etc.) 
2. When using the app for the first time, read the QR code available here to connect to the cluster:
https://selfservice.mpcdf.mpg.de/index.php?r=security

Find more info [here](https://docs.mpcdf.mpg.de/faq/2fa.html)

## Step 3: Log in ##

**If on the MPI network**
(or using a VPN to connect to one. this is required for the Bücklestraße or uni Konstanz), open the terminal and type:
```sh
ssh username@raven.mpcdf.mpg.de
```
Enter your MPCDF password and then the token generated by the 2FA app on your phone.

**If not on the MPI network**, open the terminal and type:
```sh
ssh username@gate.mpcdf.mpg.de
```
Enter your MPCDF password and then the token generated by the 2FA app on your phone.

Then, ssh into Raven:
```sh
ssh username@raven.mpcdf.mpg.de
```
Enter your MPCDF password and then the token generated by the 2FA app on your phone.

### Note on file systems ###

**File System ``` /u ```**

This is your home directory ``` $HOME ```, i.e. your default working directory when logging in to Raven and when using R. The complete path to this directory is ```/raven/u/username/ ```. The default disk quota in ``` /u ``` is 2.5 TB. 

**File System ``` /ptmp ```**

This directory is designed for batch jobs. The compete path is ```/raven/ptmp/username/ ```. Files in this directory are not backed up and will be removed if not accessed for more than 12 weeks. There is no size limit on this directory, so that the user can manage their data according to their needs. Users are required to remove all files that are not currently used. A best practice is to move your output files to your ``` /u ``` directory and then to your local machine as soon as your batch job is complete. This is done in the example files accompanying this tutorial.

## Step 4: Transfer your files ##

Raven does not have access to the files on your local machine. You need to copy the files that you need for your job (e.g. input files, scripts, etc.) to your ``` /u `` directory on Raven. 

From your **local terminal**, use the shell function copy ``` cp ``` or secure copy ```scp``` to move your files to Raven. After each copying attempt, you will be prompted to enter your MPCDF password and the 2FA token.

```sh
scp path_to_file_on_local_machine username@raven.mpcdf.mpg.de:/raven/u/username/
```

## Step 5: Load the required software packages and test your code ##

MPCDF uses environment modules to provide software packages and enable using different software versions. No modules are automatically loaded, so load the modules that require using the command ``` module load package_name/version```. For example, load the R module as follows:
```sh
module load R/4.2
```

Some other useful module commands:
```sh
module avail # see all available modules
find-module R # to locate modules and its dependencies
module avail R # see all available versions
module load R # loads the default version of R
module unload R # unload R
module purge # unload all currently loaded modules
module show R # see details of the module
module list # shows all currently loaded modules
```
Before going ahead and starting R, open a screen. A screen will allow you to go back to your last instance in case your connection to Raven is interrupted, your terminal window closes, etc. Here are some useful screen related commands:
```sh
screen -S my_screen_name # open a new screen
screen -r my_screen_name # open already existing (detached) screen 
screen -list # see a list of created screens 
screen -d -r my_screen_name # open a screen that is still attached
screen -S my_screen_name -X quit  # kill a screen or use ctrl+AK
screen ???? # detach or use ctrl+AD
```

 Now, create a new screen and open R to test your code:
 ```sh
 screen -S new_R_screen
 R
 ```
 
 Once in R, you can test your code to make sure that you can install the necessary libraries, read in your file, and overall make sure that your code (or a small version of it) works on the cluster. You will then have more confident in your code before submitting a batch job using the slurm batch system.
 
 ```R
 install.packages("tidyverse")
 install.packages("move")
 ```
 
 ## Step 6: Prepare your slurm file ##
 
 
 Overview of the available per-job resources on Raven:
```

    Job type          Max. CPUs            Number of GPUs   Max. Memory      Number     Max. Run
                      per Node               per node        per Node       of Nodes      Time
   =============================================================================================
    shared    cpu     36 / 72  in HT mode                     120 GB          < 1       24:00:00
    ............................................................................................
                      18 / 36  in HT mode        1            125 GB          < 1       24:00:00
    shared    gpu     36 / 72  in HT mode        2            250 GB          < 1       24:00:00
                      54 / 108 in HT mode        3            375 GB          < 1       24:00:00
   ---------------------------------------------------------------------------------------------
                                                              240 GB         1-360      24:00:00
    exclusive cpu     72 / 144 in HT mode                     500 GB         1-64       24:00:00
                                                             2048 GB         1-4        24:00:00
    ............................................................................................
    exclusive gpu     72 / 144 in HT mode        4            500 GB         1-80       24:00:00
    exclusive gpu bw  72 / 144 in HT mode        4            500 GB         1-16       24:00:00
   ---------------------------------------------------------------------------------------------
```

 ## Step 7: Submit your job ##
 
 ```sh
 
 sbatch your_slrm_script.slrm
 
 ```
